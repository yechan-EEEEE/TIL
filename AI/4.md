# 4. MLP (Multi-Layer Perceptron)

---

## 1. 딥러닝 (Deep Learning)

- 머신러닝 정의: 컴퓨터에게 데이터를 주고, 그 데이터 안에서 규칙(패턴)을 스스로 학습하게 하는 방식.
- 머신러닝 학습 종류:
    1.  지도 학습 (Supervised Learning): 선형/로지스틱 회귀 등.
    2.  비지도 학습 (Unsupervised Learning): K-means, PCA 등.
    3.  강화 학습 (Reinforcement Learning).

- 고전적인 머신러닝의 한계:
    1.  모델 학습을 위해 사람이 직접 데이터의 중요한 부분을 가공해서 먹여줘야 함.
    2.  데이터의 양이 일정 수준을 넘어서면, 복잡한 패턴을 추가로 학습하지 못하고 성능이 정체됨.
    3.  정형 데이터 처리에 적합하여, 이미지나 텍스트 같은 비정형 데이터 처리에 비효율적임.
    - 딥러닝은 이러한 한계를 극복하기 위해 등장한 방법론.

- 딥러닝 아이디어: 인간의 뇌에서 영감을 받은 인공신경망(ANN)을 사용하여 복잡한 패턴을 학습하는 머신러닝 학습 방법론.
    - 인공 신경망 (ANN, Artificial Neural Network): 컴퓨터 안에 인공 뉴런(퍼셉트론)을 만들고, 이들을 연결하여 네트워크를 구축한 것.

---

## 2. 퍼셉트론 (Perceptron)

- 정의: 인공 신경망(ANN)의 가장 기본적인 단위로, 여러 정보를 받아서 최종적으로 참(1) 또는 거짓(0)과 같은 결정을 내리는 간단한 결정 모델.
- 구성 요소:
    1.  입력 (inputs, x)
    2.  가중치 (weights, w)
    3.  가중합 (Weighted Sum): 입력값에 가중치를 곱한 뒤 모두 더하고 편향(Bias)을 더한 값 (`선형 회귀`와 유사).
    4.  활성화 함수 (Activation Function): 입력된 신호를 처리하여 출력 신호를 결정하는 스위치 역할.

- 특징 및 한계:
    - 퍼셉트론은 데이터 사이에 하나의 직선을 그어 두 그룹을 나누는 것과 같으며, "선형 분리 가능한 문제" (예: AND, OR 논리 게이트)는 잘 해결했음.
    - 치명적인 한계: "XOR(배타적 논리합) 문제"와 같이 하나의 직선으로 분리할 수 없는 비선형 문제를 해결할 수 없었음.
    - 극복: 퍼셉트론을 여러 층으로 쌓아 여러 개의 직선을 조합하여 XOR 문제를 해결하는 아이디어에서 신경망이 시작됨.

---

## 3. 신경망 (Neural Network)

- 얕은 신경망 (SNN, Shallow Neural Network):
    - 구조: 입력 / 은닉 / 출력의 3가지 계층으로 이루어지며, 은닉층(Hidden Layer)이 하나만 있는 구조.
    - 만능 근사 이론: 이론적으로는 하나의 은닉층만으로도 모든 연속적인 함수를 흉내 낼 수 있음.
    - 한계: 은닉층을 터무니없이 넓게 만들어야 했고 (계산량 매우 큼), 단계별/조합적 학습이 불가능했음.

- 심층 신경망 (DNN, Deep Neural Network) ≈ 딥러닝:
    - 신경망을 넓게(옆으로)가 아닌, 깊게(Deep) 쌓는 방식.
    - 여러 개의 은닉층을 가진 인공 신경망.
    - 특징: 더 복잡하고 고차원적인 특징을 학습 가능하며, 사람이 특징을 알려주지 않아도 복잡한 패턴을 스스로 발견.

---

## 4. 다중 퍼셉트론 (MLP, Multi-Layer Perceptron)

- 정의: 인공 신경망의 한 종류로, 퍼셉트론을 여러 층으로 쌓은 모델이며, 입력층과 출력층 사이에 1개 이상의 은닉층을 가짐.

- MLP의 학습 과정 (순환적 진행):
    1.  순전파 (Forward Propagation)
    2.  손실 함수 계산 (Loss Function)
    3.  역전파 (Back Propagation) 및 가중치 업데이트
    4.  1~3번 반복 후 종료 (Training 완료)

- 학습 과정 상세:
    1.  순전파 (Forward Propagation): 데이터가 입력층에서 출력층까지, 앞에서 뒤로 흘러가면서 계산되는 과정.
        - 각 층의 뉴런들은 이전 층에서 전달된 값에 가중치를 곱하고 편향을 더한 후, 활성화 함수(RELU)를 통과시켜 다음 층으로 값을 전달함. 초기에는 가중치가 랜덤이기 때문에, 예측값이 정확하지 않음.
    2.  손실 함수 계산 (Loss Function): 순전파 결과(예측값)를 실제 정답과 비교하여 손실(Loss)/오차(Error)를 구함.
        - 대표적인 손실 함수: 평균 제곱 오차 (MSE, Mean Squared Error), 교차 엔트로피 손실 (Cross-Entropy Loss) 등.
    3.  역전파 (Back Propagation): 손실값을 가지고 출력층에서 거꾸로 계산하면서, 각 가중치가 오차에 준 영향을 계산하여 오차를 줄이는 방향으로 가중치를 업데이트하는 과정.
    4.  Training 완료 시점:
        - 정해둔 학습 횟수(Epoch)에 도달하거나(적게하면 과소적합, 많이하면 과적합 우려),
        - 조기 종료 (Early Stopping): 검증 데이터로 오차를 확인하여, 오차가 더 이상 개선되지 않거나 떨어지기 시작하면 학습을 중단하는 방법 (가장 많이 쓰임).
- 역전파 진행 과정
    1. 예측값과 정답 차이에 최종 오차가 발생
    2.  출력층의 가중치들이 오차에 얼마나 기여했는지(책임 신호) 계산
    3.  출력층에서부터 입력층 방향으로 거슬러 올라가며 각 층의 가중치들이 오차에 기여한 정도를 계산
    4.  각 가중치들을 오차를 줄이는 방향으로 업데이트 (경사 하강법)
---

## 5. MLP 성능 향상 기술

MLP의 성능을 향상시키는 대표적인 기술은 다음과 같습니다:
1.  더 좋은 활성화 함수 (ReLU)
2.  더 효율적인 학습을 위한 옵티마이저 (Adam)
3.  과적합(Overfitting)을 막기 위한 규제 (Dropout)

### 5-1. 활성화 함수 (Activation Function): ReLU

- 활성화 함수 역할: 뉴런이 활성화(신호 전달)될지 여부를 결정하는 함수로, 활성화 함수가 없다면 신경망을 깊게 쌓아도 거대한 선형 함수와 다를 바 없기에, 비선형성을 부여하여 복잡한 패턴 학습 가능.
- 대표적인 활성화 함수: Sigmoid(0~1 값으로 출력), Tanh, ReLU 등.
- ReLU (Rectified Linear Unit):
    - 정의: 입력값이 0보다 작으면 0을, 0보다 크면 값을 그대로 출력.
    - 성능 향상 이유:
        1.  기울기 소실 문제 해결: 0보다 클 때 기울기가 1이므로, 역전파 과정에서 신호의 세기가 줄어들지 않고 앞쪽 층까지 온전히 전달됨 (기존의 Sigmoid 함수의 한계(기울기 소실) 극복).
        2.  매우 빠른 계산 속도: 복잡한 지수 함수 계산이 필요 없음.

### 5-2. 옵티마이저 (Optimizer): Adam

- 옵티마이저 역할: 손실 함수의 값이 최소가 되는 최적의 가중치를 찾기 위해 사용되는 알고리즘.
- 기본적인 옵티마이저: 경사 하강법 (Gradient Descent)
    - 단점: 전체 데이터셋을 사용하여 매번 가중치를 업데이트하므로 계산량이 많고, 학습률에 따라 오버슈팅이나 지역 최적점에 빠질 위험이 있음.
- Adam (Adaptive Moment Estimation): 현재 딥러닝 모델의 80% 이상이 사용하는 표준 옵티마이저로, 적응적(Adaptive)으로 학습률을 조절하는 것이 핵심.
- Adam의 아이디어:
    1.  관성 (Momentum) 도입: 이전까지 이동해 온 '속도와 방향(관성)'을 기억하여 지역 최저점을 빠져나올 수 있게 함.
    2.  적응적 학습률 도입: 가중치마다 고유한 학습률을 부여하여 최적의 학습 속도를 유지함. 많이 변경된 가중치는 학습률을 낮추고, 적게 변경된 가중치는 학습률을 높임.

### 5-3. 규제 (Regularization): Dropout

- 규제 역할: 모델의 과적합(Overfitting)을 막기 위해 제약을 거는 것.
- L1 규제(Lasso): 가중치 절대값의 합을 손실 함수에 더해 가중치를 작게 만듦. 비용 함수에 영향을 덜 주는 가중치를 0으로 만들어 특성 선택 효과도 있음. 불필요한 특성이 많을 때 유용.
- L2 규제(Ridge): 가중치 제곱합을 손실 함수에 더해 가중치를 작게 만듦. 모든 가중치를 조금씩 줄이는 효과가 있어, 과적합을 방지함. 다중공선성이 있을 때, 상관관계가 높은 특성들이 모두 선택되도록 함.
- 드롭아웃 (Dropout):
    - 정의: 학습 시 각 은닉층의 뉴런을 일정 확률(보통 0.5)로 랜덤하게 일시적으로 OFF 시키는 것.
    - 장점:
        1.  특정 뉴런이 모든 것을 학습하거나 다른 뉴런에게 지나치게 의존하는 것을 방지.
        2.  매번 다른 네트워크를 훈련시키는 것과 같은 앙상블 효과를 냄.
    - 참고: 테스트 시에는 모든 뉴런을 사용해서 테스트를 진행함.