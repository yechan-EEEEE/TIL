MCP
AI: Artificial Intelligence(인공지능)
    인간의 지능을 컴퓨터로 구현하는 것
    좁은 의미: 특정 작업을 수행하는 컴퓨터 프로그램
    넓은 의미: 인간의 지능을 흉내내는 모든 기술
    AI를 구현하는 두 가지 주요 방법
        규칙 기반 시스템(Rule-based System): 사람이 직접 규칙을 만들어서 컴퓨터가 그 규칙을 따르도록 하는 것
        머신러닝(Machine Learning): 컴퓨터에게 데이터를 주고 그 데이터 안에서 규칙(패턴)을 스스로 찾아내는 것
머신러닝 종류
    지도학습(Supervised Learning)
        정답이 있는 데이터를 학습시키는 방법
        정답(레이블, Label) 데이터(피쳐, Feature)
    지도 학습 유형
        회귀(Regression):
        연속적인 숫자 값을 예측하는 문제
        Feature와 Label을 기반으로 학습한 모델이 새로운 Feature에 대해 Label을 예측

    선형회귀(linear regression):
        예측을 하나의 직선으로 하는 가장 간단하고 직관적인 방법(다차원일 경우 평면/고차원일 경우 초평면)
        (단순 선형 회귀): 하나의 특성(Feature)만 있을 때
        w(가중치, weight): 직선의 기울기
        b(바이어스, bias): 직선의 y절편

    가설(hypothesis):
        예측에 사용할 직선의 방정식(모델)
            y = wx + b
        좋은 가설(모델) = 실제 데이터들과의 거리(오차)가 가장 작은 직선

    비용 함수(Cost Function):
        모델(가설)이 얼마나 좋은 모델인지 나타내는 점수 (=시간복잡도)
        손실 함수(Loss Function)라고도 함/ 종류:
            (회귀에서 사용)평균 제곱 오차(MSE, Mean Squared Error):
                모델의 예측값이 실제 정답과 얼마나 차이 나는지 측정하는 방법
                점수가 낮을 수록 좋은 모델
                계산 방법:
                    1. 각 데이터들의 실제값과 예측값의 오차를 구한다
                    2. 오차들을 모두 제곱하여 양수로 만들고 더한다(양수와 오차의 상쇄를 막기 위해, 큰 오차에 페널티를 부여하기 위해 제곱)
                    3. 총합을 데이터 개수로 나누어 평균을 구한다
                MSE를 0에 가장 가깝게 만드는 법 = 최소제곱법(Least Squares Method):
                    예측 모델에서 비용을 최소화하는 w(가중치)와 b(편향)을 찾는 방법
                        1. 정규 방정식(Normal Equation)
                            복잡한 미분 방정식을 풀어서 한 번에 정답을 계산해내는 방법
                            만들어진 수학공식을 이용해, 최적의 파라미터를 한 번에 계산
                            장점: 데이터의 특성이 적을 때, 빠르고 정확하게 해를 찾음
                                추가 설정이 필요 없음
                            단점: 데이터가 많으면 속도가 매우 느려짐
                                역행렬이 존재하지 않으면 계산할 수 없음(해결법: 특이값 분해, SVD:Singular Value Decomposition = 역행렬이 있는 가장 유사한 행렬을 만들기)
                                역행렬 계산 시 미세한 오차로 인해 계산 결과가 완전히 틀어질 수 있음
                        
                        2. 경사 하강법(Gradient Descent)
                            최적의 파라미터를 점진적으로 찾아나가는 방법
                            비용 함수의 기울기가 0이 되는 지점이 가장 비용(최소 제곱 오차 사용)이 낮은 지점
                            대부분의 모델이 이 방식을 채택
                                랜덤한 지점의 기울기를 구하고 그만큼 움직인다(움직이는 보폭: 학습률(learning rate))
                                반복하면서 가장 낮은 지점에 도달한다
                                학습률이 너무 크면, 보폭이 커서 반대편으로 갈 수 있음(발산: overshooting)
                                학습률이 너무 작으면, 보폭이 작아 오래 걸린다
                            용어/파라미터:
                                기울기(Gradient): 전체 데이터가 만들어내는 평균적인 오차의 기울기
                                이터레이션(iterations): 데이터의 일부 혹은 전체를 보고, 현재 위치의 경사를 계산하고, 한 걸음 이동하는 과정
                                에포크(Epoch): 전체 데이터를 모두 한 번 훑어보면 1 에포크
                                학습률(Learning Rate, alpha): 보폭
                                허용 오차(Tolerance, tol): 학습 조기 종료 조건, 모델의 오차 감소량이 매우 작아지면, 학습을 중단
                                배치 크기(batch): 기울기를 구할 때, 전체 데이터 오차의 기울기를 한 번에 구하지 않음/ 작은 묶음(배치)로 나눠서 처리/ 전체 데이터 오차의 기울기를 한 번에 구하면 1iterations = 1epoch
                                기울기 누적 스텝(Gradient Accumulation Steps): 실질적인 배치 크기를 늘리는 고급 기법, 매 배치마다 업데이트하지 않고, n번의 배치 간격으로 누적한 기울기를 기반으로 이동
                            1. 배치 경사 하강법(Batch Gradient Descent)
                                전체 데이터를 훑어보고, 기울기를 계산
                                정확하지만, 데이터가 크면 매우 느림
                            2. 확률적 경사 하강법(Stochastic Gradient Descent)
                                하나의 데이터만 보고 기울기를 계산
                                매우 빠르지만, 불안
                            3. 미니배치 경사 하강법(Mini-batch Gradient Descent)
                                적당한 수의 배치(batch)만 보고 기울기를 계산, 속도/ 안정성 모두를 잡음

    다항회귀(Polynomial Regression):
        선형 회귀의 확장판

다항회귀(polynomial regression):
    독립 변수의 차수를 높여 비선형 관계를 모델링하는 방법
    예: 2차 다항식 y = ax^2 + bx + c
    다항회귀는 선형 회귀 모델을 사용하므로, 비용 함수와 최적화 방법은 선형 회귀와 동일

분류(Classification)
로지스틱회귀(logistic regression):
    선형 회귀 + 시그모이드 함수 + 이진 교차 엔트로피(Binary Cross Entropy)
    이진 분류 문제 해결 방법 = 시그모이드 함수:
        데이터를 특정 그룹으로 나누는 이진 분류 방법
        평균 제곱 오차 대신 이진 교차 엔트로피(Binary Cross Entropy)를 사용
            이진 분류 문제를 위한 전용 손실 함수
            틀린 답에 대해 모델이 얼마나 강하게 확신했느냐에 따라 벌점의 크기를 기하급수적으로 늘리는 것
    다중 분류 문제 해결 방법:
        소프트맥스 회귀(Softmax Regression)
        선형 회귀 + 소프트맥스 함수(Softmax) + 범주형 교차 엔트로피(Categorical Cross Entropy)
            소프트맥스(Softmax) 함수
            모든 클래스에 대한 확률의 총합이 반드시 1이 되도록 만들어주는 함수
        (회귀)평균 절대 오차(MAE, Mean Absolute Error)

데이터 전처리
    모델이 학습을 잘 할 수 있도록 데이터를 정돈된 상태로 바꾸는 모든 과정
    1. 결측치: 비어있는 값(NaN)은 수학 계산을 불가능하게 만듦
        처리 방법
        1: 삭제(Deletion) 간단하고 빠르지만 소중한 데이터를 잃어버릴 수 있음(데이터가 적을 때는 치명적)
        2: 대치(Imputation) 결측치를 다른 값으로 채우는 방법
            평균/중앙값/최빈값 대치: 수치형 데이터에 주로 사용, 간단하고 빠르지만 데이터의 분포를 왜곡할 수 있음
            예측 대치: 결측치를 예측하는 모델을 만들어서 채우는 방법, 더 정확하지만 복잡하고 시간이 많이 걸림
            고급 대치: KNN 대치, 다중 대치 등, 더 정교한 방법이지만 구현이 복잡함
    2. 문자열/ 범주형 데이터: 같은 문자열을 직접 계산할 수 없음
        처리 방법
        컴퓨터가 알아들을 수 있도록 문자 데이터를 숫자로 변환하는 과정
    3. 스케일링: 단위와 범위가 전혀 다른 데이터가 섞이면, 컴퓨터는 데이터의 중요성을 판단할 수 없음
        표준화(선호): 제각각인 데이터의 단위를 공평하게 맞춰주는 작업/ 데이터의 평균을 0, 표준편차를 1로 맞춤/ 이상치에 덜 민감/ 데이터의 분포를 유지/ 경사 하강법, SVM, KNN 등 거리 기반 알고리즘에 유용
        정규화: 흩어져 있는 모든 데이터의 값을 0과 1 사이의 범위로 변환
        데이터의 범위를 명확하게 제한하고 싶은 경우에 활용/ 데이터의 최솟값 최대값을 명확히 알면 유용함, 이상치에 민감
