# 데이터 EDA
- 탐색적 데이터 분석(EDA, Exploratory Data Analysis)
    - 데이터를 본격적으로 분석하기 전에 여러 각도에서 관찰하고 이해하는 과정
    - 데이터를 깊이 이해하는 모든 프로젝트의 첫 과정
    - 중요성
        1. 데이터 이해: 데이터의 특성과 구조를 파악, 변수 간의 관계와 패턴을 발견
        2. 문제 정의: 분석 목표와 문제를 명확히 설정, 올바른 질문을 던질 수 있도록 도움
        3. 데이터 품질 평가: 결측치, 이상치, 중복 데이터 등 데이터의 품질 문제를 발견하고 해결
        4. 모델 선택 및 개선: 적절한 분석 방법과 모델을 선택, 데이터의 분포, 패턴, 이상치, 결측치 등을 파악
        5. 의사결정 지원: 데이터 기반의 의사결정을 내리는 데 도움, 분석 결과를 시각화하여 이해관계자와 소통
    - 탐색적 데이터 분석 과정
        1. 데이터 수집: 다양한 출처에서 데이터를 수집
        2. 데이터 정리: 결측치 처리, 이상치 제거, 데이터 형식 변환 등 데이터 전처리
        3. 데이터 시각화: 히스토그램, 박스플롯, 산점도 등 다양한 시각화 기법을 활용해 데이터의 분포와 관계를 파악
        4. 통계적 분석: 기초 통계량(평균, 중앙값, 분산 등)을 계산하고, 변수 간의 상관관계 분석
- 상관관계: 두 변수 간의 관계를 나타내는 통계적 지표
- 인과관계: 한 변수가 다른 변수에 영향을 미치는 관계
- 아이스크림 판매량 = 상어 공격 횟수 증가: 상관관계 O 인과관계 X

데이터의 특성 파악
    1. 데이터 타입: 수치형(연속형, 이산형), 범주형(명목형, 서열형)
    2. 기초 통계량: 평균, 중앙값, 최빈값, 분산, 표준편차, 사분위수
    3. 분포 확인: 히스토그램, 밀도 함수, 박스 플롯

단변수(데이터 타입이 하나) 분석
    수치형 데이터: 히스토그램, 박스플롯
    범주형 데이터: 막대그래프, 파이차트

---

# 데이터 EDA (탐색적 데이터 분석)

## 1. 탐색적 데이터 분석 (EDA, Exploratory Data Analysis)

- 정의: 데이터를 본격적으로 분석하기 전에 여러 각도에서 관찰하고 이해하는 과정이며, 모든 프로젝트의 첫 과정.
- 중요성:
    1. 데이터 이해: 데이터의 특성과 구조를 파악, 변수 간의 관계와 패턴을 발견
    2. 문제 정의: 분석 목표와 문제를 명확히 설정, 올바른 질문을 던질 수 있도록 도움
    3. 데이터 품질 평가: 결측치, 이상치, 중복 데이터 등 데이터의 품질 문제를 발견하고 해결
    4. 모델 선택 및 개선: 적절한 분석 방법과 모델을 선택, 데이터의 분포, 패턴, 이상치, 결측치 등을 파악
    5. 의사결정 지원: 데이터 기반의 의사결정을 내리는 데 도움, 분석 결과를 시각화하여 이해관계자와 소통
- 진행 순서 (반복적 진행):
    1.  데이터를 불러오고, 기본 정보 확인 (타입, 결측치, 통계량 등).
    2.  데이터 정제 및 전처리 (결측치, 이상치, 중복값 처리, 데이터 타입 수정, 불필요한 변수 제거).
    3.  개별 변수 분석 (분포와 특징 이해).
    4.  변수 간 관계 분석 (관계, 패턴, 상관관계 파악).
    5.  분석 결과 정리.
- 상관관계: 두 변수 간의 관계를 나타내는 통계적 지표.
두 변수가 함께 변화하는 관련성을 의미하며, 일반적으로 -1에서 1 사이의 값(상관계수)을 가집니다.
    - 양의 상관관계(1에 가까움): 한 변수 증가 시 다른 변수도 증가.
    - 음의 상관관계 (-1에 가까움): 한 변수 증가 시 다른 변수는 감소.
    - 상관관계 없음 (0에 가까움): 뚜렷한 선형 관계가 없음.
- 인과관계: 한 변수가 다른 변수에 영향을 미치는 관계.
- 예시: 아이스크림 판매량과 상어 공격 횟수는 여름철에 함께 증가하는 경향이 있어 상관관계가 있지만, 아이스크림 판매량이 상어 공격 횟수에 영향을 미치지 않으므로 인과관계는 없습니다.
    - 주의: 상관관계가 높다고 해서 원인과 결과(인과관계)를 의미하는 것은 아닙니다.

---

## 2. 데이터 시각화

Matplotlib과 Seaborn 라이브러리를 활용하여 데이터 패턴을 직관적으로 파악합니다.
- Matplotlib: 기본적인 그래프 작성에 유용한 라이브러리.
- Seaborn: 통계적 그래프 작성에 특화된 라이브러리

### 단변수 분석 (Univariate Analysis)

히스토그램 (Histogram): 데이터의 전체 범위를 여러 구간(bin)으로 나누어 각 구간에 속하는 데이터의 빈도를 막대그래프로 나타냄.
- 목적: 수치형 변수 하나의 데이터 분포 파악.
- 장점: 분포 파악이 직관적, 이상치 탐지 수월.
- 단점: 구간(bin) 설정에 민감.

박스 플롯 (Box Plot):
- 목적: 수치형 데이터의 분포와 이상치를 한눈에 파악 (사분위수 이용).
- 장점: 분포와 이상치 파악에 유용.
- 단점: 데이터 개수 파악/구체적인 분포 형태 알기 어려움.

막대 그래프 (Bar Plot):
- 목적: 범주형 변수의 빈도나 비율 파악.
- 장점: 각 범주의 크기를 직관적으로 비교 가능.
- 단점: 범주 수가 많아지면 복잡해질 수 있음.

### 다변수 분석 (Multivariate Analysis)

산점도 (Scatter Plot): 두 개의 수치형 변수 사이의 관계 파악을 위해 점을 찍어 만드는 그래프
- 장점: 패턴 및 이상치 발견에 용이
- 단점: 인과관계를 증명하지 않음.
히트맵 (Heatmap): 여러 수치형 변수들 간의 크기를 색상으로 변환 (주로 상관관계 분석).
- 장점: 다변량 관계를 직관적으로 파악, 상관관계가 높은 변수 쌍 찾기에 유용.
- 단점: 정확한 수치 파악 어려움, 변수가 많아지면 복잡해짐.
페어 플롯 (Pair Plot): 여러 변수들 간의 관계와 분포를 한 번에 보여줌.
- 대각선(분포도): 각 성분이 어떻게 분포하는지 히스토그램으로 표시
- 비대각선(삼전도): 변수 쌍 간의 산점도로 관계 표시
- 장점: 짧은 코드로 모든 변수 간의 관계와 분포를 동시 탐색 가능.
- 단점: 변수 개수가 많아지면 복잡해짐.

---

## 3. 모델 학습 및 과적합/과소적합

 과적합 (Overfitting): 모델이 훈련 데이터의 사소한 잡음까지 외워버려, 훈련 데이터에서는 성능이 좋으나 새로운 테스트 데이터에서는 성능이 낮아지는 현상.
 과소적합 (Underfitting): 모델이 너무 단순하여 훈련 데이터의 핵심 패턴조차 제대로 학습하지 못한 상태. 훈련/테스트 데이터 모두에서 성능이 낮음.

### 데이터 분리 및 전처리

1.  데이터 분리 (Data Splitting): 주어진 데이터를 훈련 데이터 (모델 학습)와 테스트 데이터 (일반화 성능 최종 평가, 과적합 확인)로 분리합니다.
    일반적으로 7:3 또는 8:2 비율로 분리.
    train_test_split(): 주어진 데이터를 훈련 데이터와 테스트 데이터로 나누는 함수
    test_size: 테스트 데이터의 비율을 지정하는 매개변수 (예: 0.2는 20%를 테스트 데이터로 사용).
    random_state: 데이터 분할 시드 값으로, 동일한 시드를 사용하면 매번 같은 결과를 얻을 수 있습니다.
    stratify=y: 원본 데이터의 정답 비율을 분할된 데이터셋에 동일한 비율로 나눠주는 옵션
    훈련 데이터에 '정상'메일만 들어가고, 테스트 데이터에 '스팸'메일만 들어가는 것을 방지할 수 있음
    분류 문제에서 필수 옵션
2.  스케일링 (Scaling): 각 특성(변수)의 단위를 맞춰 모델이 공평하게 학습하도록 만드는 과정입니다.
    표준화 (Standardization): 데이터의 평균을 0, 표준편차를 1로 변환합니다.
    주의: 테스트 데이터의 표준화는 반드시 훈련 데이터의 평균/표준편차를 활용해야 데이터 유출(Data Leakage)을 방지할 수 있습니다.

---

## 4. 분류 모델 성능 평가

모델의 예측값과 실제 정답을 비교하여 성능을 객관적인 숫자로 확인하는 과정입니다.

### 주요 평가지표

- 정확도 (Accuracy): (맞은 예측 개수) / (전체 예측 개수).
가장 직관적이나, 데이터 불균형이 심할 경우 (예: 암 진단) 성능 지표로 사용하기 어렵다.
- 혼동 행렬 (Confusion Matrix): 모델이 어떤 실수를 했는지 상세하게 보여주는 도구 (TP, TN, FP, FN).
- 정밀도 (Precision): 모델이 "Positive"라고 예측한 것들 중 진짜 "Positive"인 비율 (모델의 예측이 진짜라고 믿을 수 있을 때 유용).
- 재현율 (Recall): 실제 "Positive" 중에서 모델이 "Positive"라고 맞춘 비율 (모델이 하나라도 놓치면 안될 때 유용).
- 조화평균 (F1-Score): 정밀도와 재현율을 모두 고려한 균형 잡힌 성능 지표이며, 클래스 불균형이 심할 때 정확도를 대체할 수 있습니다.(정밀도를 올리면 재현율이, 재현율을 올리면 정밀도가 떨어지는 경향이 있음, Trade-off 관계)

---

## 5. 성능 검증 (Validation)

모델 학습 시 설정하는 하이퍼파라미터 (예: 학습률)에 따라 점수가 달라질 수 있으므로, 최적의 방식을 찾기 위해 검증을 진행합니다. 검증을 테스트 데이터로 진행하면 "데이터 유출(Data Leakage)"이 발생합니다.

- 검증 데이터 (Validation Data): 모델 훈련 과정에서 최고의 모델/훈련 방법을 선택하기 위해 학습 데이터에서 따로 분리하여 사용되는 데이터.
- 교차 검증 (Cross-Validation): 훈련 데이터를 여러 개의 덩어리(Fold)로 나눈 뒤, 각 Fold가 돌아가면서 한 번씩 '검증 데이터' 역할을 하는 방법. 일회성 검증의 불안정성을 극복합니다.

---

## 6. 비지도 학습 (Unsupervised Learning)

정답이 없는 데이터의 숨겨진 구조나 규칙을 찾는 방법.

### 군집화 (Clustering)

정답이 없는 데이터들을 유사한 특징을 가진 것끼리 하나의 그룹(Cluster)으로 묶는 작업.
정답이 없어도 되기 때문에 레이블링 작업이 필요 없어 비용이 적게 듭니다.
주요 방법:
- K-평균 (K-Means): 데이터들이 가장 가까운 중심점(Centroid)을 기준으로 그룹을 형성하도록 합니다.
    1. 초기화: K개의 중심점(centroid)을 무작위로 선택
    2. 할당 단계: 각 데이터를 가장 가까운 중심점에 할당
    3. 업데이트 단계: 각 군집의 중심점을 재계산
    4. 수렴 확인: 중심점 변화가 없을 때까지 2-3단계를 반복
    - 이상치에 민감: 이상치가 중심점에 큰 영향을 미쳐 초기 중심점 위치에 따라 결과가 왜곡될 수 있다.
    - 군집의 개수(K) 사전 지정 필요: 적절한 K 값을 선택하는 것이 중요하며, 잘못된 K 값은 부적절한 군집화를 초래할 수 있다.
    - 엘보우 방법 (Elbow Method): K값을 1부터 10까지 늘려가며 적절한 K값을 선택하는 방법으로, 이너셔(데이터와 중심점 거리의 총합)가 급격하게 감소하다 완만해지는 지점을 최적의 K 후보로 선택합니다.(이너셔가 작을 수록 군집이 잘 뭉친 것)
   - 평가 = 실루엣 스코어 (Silhouette Coefficient): 각 데이터들이 다른 그룹과의 거리에 따라서 평가합니다.
- 계층적 군집 (Hierarchical Clustering): K-Means와 달리 K 값을 미리 정하지 않아도 되며, 가까운 것부터 순서대로 묶어 나무 형태의 구조를 만듭니다.

### 차원 축소 (Dimensionality Reduction)

- 필요성: 데이터의 특징이 너무 많아지면 "차원의 저주(Curse of Dimensionality)"가 발생하여 모델 성능 저하, 과적합, 계산 시간 증가, 시각화 어려움 등의 문제가 생깁니다.
- 정의: 데이터의 중요한 정보는 유지하면서, 불필요한 특징을 제거하거나 압축하여 차원을 줄이는 기술입니다.
- 주요 방법:
    1. 특성 선택 (Feature Selection): 여러 특징 중 중요하다고 생각되는 몇 개의 특징만 골라내는 방식.
    2. 특성 추출 (Feature Extraction): 기존 특징들을 새로운 축으로 투영하여 더 적은 수의 새로운 특징으로 압축하는 방식. ex): PCA (주성분 분석, Principal Component Analysis): 데이터가 가장 넓게 퍼져있는 (분산이 가장 큰) 방향으로 새로운 축(주성분)을 찾는 기술. 주성분과 직각을 이루는 축이 두 번째 주성분이 되고, 이런 식으로 여러 개의 주성분을 찾습니다.

## 7. 참고
- ROC 곡선과 AUC
    - ROC 곡선 (Receiver Operating Characteristic Curve): 이진 분류 모델의 성능을 평가하는 그래프로, 다양한 임계값에서의 TPR(재현율)과 FPR(위양성률)을 나타냅니다.
    - AUC (Area Under the Curve): ROC 곡선 아래 면적을 의미하며, 0.5에서 1 사이의 값을 가집니다. 1에 가까울수록 모델 성능이 우수함을 나타냅니다.