# 5 토큰화와 임베딩 (Tokenization & Embedding)

---

## 1. NLP 모델의 학습 과정 (큰 그림)

사람의 문장이 딥러닝 모델에 입력되기까지의 과정은 다음과 같습니다:
1.  토큰화 (Tokenization): 문장을 의미 있는 단위(토큰)로 분리합니다.
2.  정수 인코딩 (Integer Encoding): 각 토큰을 고유한 정수 ID로 변환합니다. 
3.  임베딩 (Embedding): 정수 ID를 단어의 의미를 담은 고차원 벡터로 변환합니다.
4.  딥러닝 모델 입력: 최종 임베딩 벡터가 RNN, LSTM, Transformer 등 딥러닝 모델의 입력으로 사용됩니다. 

---

## 2. 토크나이저 (Tokenizer)

-   정의: 문장을 토큰(Token)이라는 의미 있는 단위로 나누고, 각 토큰을 정수(ID)로 변환하는 도구입니다. 
-   역할: 컴퓨터가 글자를 이해하기 위한 첫 번째 번역 작업입니다. 
-   예시:
    * 문장: "안녕하세요" 
    * 토큰화: `["안녕", "하세요"]` 
    * 정수 인코딩: `[8192, 91352]` 

### 2-1. 허깅페이스(Hugging Face)를 활용한 토큰화 실습

허깅페이스(Hugging Face)란?
-   최신 NLP 딥러닝 기술을 손쉽게 사용할 수 있도록 도와주는 플랫폼이자 라이브러리입니다. 
-   `Transformers` 라이브러리는 모델, 토크나이저 등을 쉽게 로드하고 사용할 수 있도록 합니다. 

주요 도구:
-   `AutoTokenizer`: 모델 이름("klue/bert-base")만 알려주면, 해당 모델에 맞는 토크나이저를 자동으로 찾아주는 편리한 기능

실습 코드 흐름:
1.  라이브러리 로딩: `from transformers import AutoTokenizer` 
2.  토크나이저 로드: `tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")` 
3.  문장 인코딩: `encoded_input = tokenizer(text)` 

인코딩 결과 분석:
-   `input_ids`: 각 토큰에 해당하는 정수 ID 리스트로, 가장 중요한 결과값입니다. 
-   스페셜 토큰:
    * `[CLS]`, `[SEP]`: 문장의 시작과 끝을 알리는 스페셜 토큰입니다. 
-   서브워드 (Subword):
    * 예시: `##하세요`, `##은` 
    * 특징: 단어가 더 작은 단위로 나뉜 것이며, OOV(Out-Of-Vocabulary, 모르는 단어) 문제를 해결하는 데 효과적입니다.

---

## 3. 임베딩 (Embedding)

-   정의: 토큰에 부여된 정수 ID를, 의미를 함축한 고차원의 벡터(Vector)로 변환하는 과정입니다. 
-   필요성: 정수 ID는 단어 간의 관계를 표현할 수 없는 단순한 숫자이지만, 임베딩 벡터는 단어의 의미, 문맥, 관계성을 좌표값으로 표현합니다.
-   특징:
    * 의미가 비슷한 단어들은 벡터 공간에서 가까운 위치에 존재합니다. 
    * 단어의 의미적 관계를 벡터 연산으로 표현할 수 있습니다. 
        > 예: 벡터("왕") - 벡터("남자") + 벡터("여자") ≈ 벡터("여왕") 
    * 임베딩은 모델이 단어의 유사도와 패턴을 학습할 수 있게 하며, 딥러닝 모델의 실제 입력값이 됩니다. 

### 3-1. 임베딩 벡터 추출 실습

임베딩 벡터는 사전 학습된 모델을 통해 추출합니다.

실습 코드 흐름:
1.  모델 로딩: `from transformers import AutoModel`을 사용하여 `klue/bert-base`와 동일한 이름의 모델을 로드합니다. 
2.  모델 입력: 토크나이저를 사용해 텍스트를 인코딩한 후, 이 인코딩된 입력을 모델에 전달합니다. 
    `output = model(encoded_input)` 
3.  결과 추출: `output.last_hidden_state`가 최종 임베딩 벡터입니다. 

임베딩 결과 Shape 분석 (예: `torch.Size([1, 26, 768])`) 
| 위치 | 이름 | 의미 |
| :--- | :--- | :--- |
| 1 | Batch Size | 한 번에 처리한 문장의 개수입니다.  |
| 26 | Sequence Length | 문장을 토큰화했을 때의 총 토큰 개수입니다.  |
| 768 | Hidden Size / Embedding Dimension | 하나의 토큰을 표현하는 데 사용된 숫자의 개수(벡터의 차원)입니다. 즉, 26개의 모든 토큰이 각각 768개의 숫자로 이루어진 벡터로 변환되었음을 의미합니다.  |

---
# 5.1 RNN과 LSTM 개요

- RNN (Recurrent Neural Network): 순차적 데이터를 처리하기 위해 설계된 신경망으로, 과거의 정보를 기억하고 활용하는 구조입니다.
- LSTM (Long Short-Term Memory): RNN의 한 종류로, 장기 의존성 문제를 해결하기 위해 셀 상태와 게이트 구조를 도입한 모델입니다.

---

## 1. 순차적 데이터 (Sequential Data) 

-   정의: 순서에 의미가 있는 모든 종류의 데이터입니다. 데이터의 순서가 바뀌면 그 의미나 정보가 완전히 달라집니다.
-   특징:
    * 순서가 중요합니다.
    * 장기 의존성(Long-term dependency): 과거의 정보가 현재/미래에 영향을 줍니다.
    * 가변 길이: 데이터의 길이가 고정되어 있지 않은 경우가 많습니다 (예: 문장의 길이, 음악의 길이).
-   대표적인 예시: 자연어(텍스트), 시계열 데이터(날씨 데이터, 주식 가격), 오디오/비디오 데이터.

---

## 2. 다중 퍼셉트론 (MLP)의 한계와 RNN의 등장 

-   MLP의 한계:
    * 입력 데이터의 순서를 고려하지 못함: "I go to school"과 "School go to I"를 동일한 정보로 처리하여, 단어의 순서에 담긴 의미를 학습할 수 없습니다.
    * 고정된 입출력 크기: 모델을 설계할 때 입력과 출력의 크기(차원)가 고정되어야 하며, 문자열이나 순차적 데이터의 길이가 달라지면 처리하기 힘듭니다.
-   RNN의 등장: 위의 한계를 극복하기 위해 RNN(Recurrent Neural Network)이 등장했습니다.

---

## 3. RNN (Recurrent Neural Network) 

-   정의: 순환 신경망이라고 하며, 순서가 있는 데이터를 처리하기 위해 고안된 인공 신경망입니다.
-   특징: 과거의 정보를 기억하고, 기억한 정보를 활용해 현재의 데이터를 해석합니다.

### 3-1. RNN Cell의 작동 방식

RNN은 "Cell"을 재사용하며, 이 Cell은 입력 데이터와 과거의 정보를 섞어 새로운 정보를 만들어냅니다.

1.  입력 데이터 받기:
    * 현재 데이터($x_t$)
    * 이전 Cell의 은닉 상태($h_{t-1}$): 과거의 정보를 담아두는 '기억'입니다.
2.  가중치 반영: 셀은 두 가지 데이터에 각각의 가중치를 곱해 중요도를 반영합니다:
    * 입력 가중치($W_{xh}$): 현재 데이터의 영향력.
    * 순환 가중치($W_{hh}$): 이전 기억의 영향력.
3.  정보 조합: 가중치가 반영된 현재 정보와 과거 기억을 합칩니다.
4.  활성화 함수 적용: 합쳐진 정보를 `tanh` 활성화 함수에 넣어 압축합니다.
5.  새로운 은닉 상태 전달: 압축된 새로운 은닉 상태($h_t$)는 다음 Cell로 전달됩니다.

### 3-2. RNN의 학습 방식: BPTT

-   역전파 (Backpropagation): RNN에서는 시간을 거슬러 오차를 전파한다고 하여 BPTT (Backpropagation Through Time)라고 부릅니다.
-   가중치 공유: RNN은 모든 시점에서 동일한 Cell을 재사용하고 동일한 가중치($W_{xh}, W_{hh}$)를 공유합니다.
    * 학습 시 모든 시점의 가중치들을 합산하여 오차가 줄어드는 방향으로 한 번에 업데이트합니다.
    * 이러한 가중치 공유 덕분에 모델이 가볍고 효율적이며, 훈련 때 학습하지 않은 길이의 문장에도 유연하게 대처할 수 있습니다.

### 3-3. RNN의 한계: 기울기 소실 문제

-   기울기 소실 문제 (Vanishing Gradient Problem): BPTT 과정에서 오차 정보가 이전 정보와 현재 시점의 책임으로 계속 나눠지면서, 뒤로 갈수록 오차 정보가 거의 전달되지 않아 오래된 정보에 대해 학습이 제대로 이루어지지 않습니다.
-   결과: 장기 의존성 문제 (Long-Term Dependency Problem)가 발생합니다.

---

## 4. LSTM (Long Short-Term Memory)

-   정의: RNN의 한 종류로, 기존 RNN의 장기 의존성 문제를 해결하기 위해 설계된 모델입니다.
-   해결 방법: 셀 상태(Cell State)와 게이트(Gate)라는 독자적인 구조를 도입했습니다.

### 4-1. LSTM의 핵심 구조

| 구분 | 기존 RNN | LSTM |
| :--- | :--- | :--- |
| 정보 전달 통로 | 은닉 상태($h_t$) 하나만 사용 | 은닉 상태($h_t$) + 셀 상태($C_t$) 두 가지 통로 활용 |
| 셀 상태 ($C_t$) | - | 장기 기억을 전달하는 통로. 단순하고 직접적인 경로로써 복잡한 변환을 거치지 않아 기울기가 소실되지 않고 시간을 거슬러 올라갈 수 있게 합니다. |
| 게이트 (Gate) | - | 셀 상태를 실질적으로 컨트롤하며, 어떤 정보를 기억/망각/사용할지 스스로 학습하여 결정합니다. |

### 4-2. LSTM의 세 가지 게이트

게이트는 모두 시그모이드 함수를 활용하여 0과 1 사이의 값으로 출력하며, 정보의 흐름을 조절합니다.

1.  망각 게이트 (Forget Gate):
    * 역할: 과거의 기억(장기 기억)에서 불필요한 정보를 제거할지 결정합니다.
    * 작동: 현재 상황을 고려하여 "망각 벡터"를 생성하고, 이를 셀 상태에 곱하여 선택적으로 기억을 지웁니다.
2.  입력 게이트 (Input Gate):
    * 역할: 새로운 정보 중 어떤 것을 셀 상태(장기 기억)에 저장하고 업데이트할지 결정합니다.
    * 작동: 새로운 정보와 업데이트 벡터를 조합하여 셀 상태에 업데이트합니다.
3.  출력 게이트 (Output Gate):
    * 역할: 셀 상태(장기 기억)의 모든 정보 중에서 예측에 필요한 정보만 선별하여 외부로 내보내는 최종 결과를 결정합니다.
    * 작동: "출력 벡터(마스크)"를 생성하고, 이를 셀 상태에 곱하여 중요한 데이터만 출력합니다. (이 과정은 셀 상태를 변경하지는 않습니다).

### 4-3. LSTM의 학습과 한계

-   기울기 소실 문제 해결: 셀 상태($C_t$)는 복잡한 활성화 함수를 거치지 않고 단순 덧셈과 곱셈 연산만 진행했기 때문에, 역전파 시 미분값이 '1'이 되어 오차 값이 손실 없이 계속 전달됩니다. 이로 인해 기울기 소실 문제가 해결됩니다.
-   여전한 한계:
    * LSTM은 하나의 Cell을 재사용하고 있어 입력 길이는 달라져도 상관없지만, 입력값에 대응하는 출력값도 1개가 나옵니다.
    * 이 문제를 해결하기 위해 Seq2Seq (Sequence to Sequence) 모델이 등장했으며, 이는 입력받는 역할(인코더)과 출력하는 역할(디코더)을 나눠 유연하게 대처할 수 있도록 했습니다.

---
## 참고 자료
- https://www.youtube.com/watch?v=PahF2hZM6cs
- https://wikidocs.net/22647
gpt한테 숫자로 과정 보여달라 하기