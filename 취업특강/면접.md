---------------------
AI 챌린지 프로젝트에서 본인이 한 핵심 역할은?

답변
저는 모델 구조 비교, 파인튜닝을 실험하고 성능 개선을 담당했습니다.
후에 결과물을 분석하고 취합하여 발표를 담당했습니다.
발표 후에 질의응답을 위해 프로젝트의 전반적인 부분도 숙지했습니다.
--------------------------------------------------------------------------------------------------

1분 자기소개
안녕하세요.
저는 멀티모달 AI 모델 실험과 성능 개선에 강점을 가진 예비 개발자 문예찬입니다.

SSAFY 교육 과정에서 Python 기반 모델링, 데이터 증강, 파인튜닝 등 AI 개발 과정을 경험했고,
특히 이미지 기반 VQA 챌린지 프로젝트에서 반복 실험으로 정답률 93%를 달성하며
실험 중심 개발 문화가 저와 잘 맞는다는 것을 확인했습니다.

저는 웹툰을 매우 좋아해 평소 작가님들의 작업 환경 개선에 관심이 많았고,
툰스퀘어가 ‘AI로 창작자의 부담을 줄인다’는 방향성에 깊이 공감해 지원하게 되었습니다.

입사 후에는 이미지 생성 및 멀티모달 모델의 성능 고도화,
그리고 데이터 기반 품질 개선을 통해
웹툰 제작 도구에서 창작자에게 실질적 가치를 주는 모델을 만들고 싶습니다.
감사합니다.
--------------------------------------------------------------------------------------------------
1 — 왜 툰스퀘어인가? (지원 동기)

답변 예시
저는 웹툰을 오랫동안 즐겼고, 창작자의 작업비용과 시간 부담이 크다는 문제를 느껴왔습니다.
툰스퀘어가 “생성형 AI로 창작자의 장벽을 낮춘다”를 명확한 미션으로 가져가는 점이 가장 인상 깊었습니다.

AI 챌린지에서 멀티모달 모델을 성능 개선하며 “AI로 실제 문제를 해결한다”는 경험을 했고,
툰스퀘어의 빠른 실험 문화와 결과 중심 접근이 제가 추구하는 방식과 맞다고 판단해 지원했습니다.
--------------------------------------------------------------------------------------------------
AI가 웹툰 제작에 적용될 때 가장 중요한 요소는?

답변
사용자 입장에서 순간적으로 느려서는 안 되고,
디자이너를 대체한다기보다 창작자가 반복 작업을 줄여주는 방식으로 존재해야 합니다.
성능 자체보다 일관성, 시간 절약, 수정 용이성이 더 큰 가치라고 생각합니다.
--------------------------------------------------------------------------------------------------
웹툰 생성 모델과 이미지 생성 모델 차이는 뭐라 생각하나요?

답변
일반 이미지 생성은 단일 장면 최적화,
웹툰은 컷 간 스타일 연결성과 장면 스토리 전개가 필요합니다.
즉, 단순 품질보다 컨텍스트 일관성과 반복 재생산의 안정성이 핵심 과제입니다.
--------------------------------------------------------------------------------------------------
AI 경험이 실무만큼 깊지 않은데 어떻게 보완할 건가요? (압박 질문)

답변
한 프로젝트에서 end-to-end를 경험했다는 것을 강점이라 생각합니다.
처음부터 끝까지 경험했기 때문에
문제가 생기면 어느 부분을 건드려야 하는지 흐름을 이해하고 있습니다.
--------------------------------------------------------------------------------------------------
PyTorch로 모델 학습 시 기본 흐름을 설명해보세요.

Dataset과 DataLoader로 데이터를 구성하고,
모델 정의 후 loss와 optimizer를 설정합니다.
학습 루프에서 forward → loss 계산 → backward(역전파) → optimizer step을 반복하며,
validation 단계에서는 torch.no_grad()로 성능을 평가합니다.
--------------------------------------------------------------------------------------------------
Python으로 AI 개발할 때 중요하게 생각하는 점은?

가독성과 재현성입니다.
실험이 반복되기 때문에 코드 구조를 단순하게 유지하고,
실험 설정을 쉽게 바꿀 수 있도록 관리하는 것이 중요하다고 생각합니다.
--------------------------------------------------------------------------------------------------
Docker/AWS 경험 없는데 왜 지원했나요?

아직 실무 경험은 없지만,
모델 학습과 실험 경험을 통해 배포까지 이어지는 흐름을 이해하고 있습니다.
신입으로서 기존 파이프라인을 빠르게 학습해 적용하는 것이 제 역할이라고 생각합니다.
--------------------------------------------------------------------------------------------------
최신 모델을 “깊이 이해”한다고 할 수 있나요?

모든 구현을 다 해봤다고 말할 수는 없지만,
구조와 설계 의도를 이해하고, 문제에 맞게 선택할 수 있는 수준이라고 생각합니다.
실무에서는 그 이해를 바탕으로 빠르게 학습하고 적용하겠습니다.
--------------------------------------------------------------------------------------------------
AI 챌린지에서 데이터 전처리

이미지 쪽은 의미를 해치지 않는 최소한의 증강만 적용했고,
텍스트 쪽은 질문 템플릿과 선택지 형식을 최대한 통일했습니다.
특히 선택지 길이와 표현 차이가 모델 편향으로 이어질 수 있어 이를 정규화했고,
데이터 수가 적어 번역–역번역 방식으로 증강해 표현 다양성을 확보했습니다.
이 전처리가 성능 향상에 가장 큰 영향을 줬습니다.
--------------------------------------------------------------------------------------------------
텍스트 불필요한 공백 제거
특수문자 통일
질문 포맷 통일
선택지 정규화
--------------------------------------------------------------------------------------------------
✅ 1. LLM / Transformer / Attention (통합)
❓ Q. Transformer와 Self-Attention을 설명해보세요. (RNN과의 차이 포함)
✅ 정리된 답변 (이것만 외우면 됨)

기존 RNN이나 LSTM은 단어를 순차적으로 처리해 병렬화가 어렵고, 긴 문장에서 정보 손실이 발생했습니다.
Transformer는 이를 해결하기 위해 Self-Attention 기반 인코더-디코더 구조를 사용해 모든 토큰을 동시에 처리합니다.

Self-Attention은 각 토큰을 Query, Key, Value로 변환한 뒤
Query와 Key의 유사도를 계산해 어떤 단어를 얼마나 참고할지 가중치를 구하고,
이를 Value에 적용해 문맥을 반영한 새로운 표현을 만듭니다.

이 구조 덕분에 문장 전체 문맥을 효율적으로 반영할 수 있고,
병렬 처리가 가능해 대규모 모델 학습에 적합합니다.
--------------------------------------------------------------------------------------------------
LLM은 Transformer 기반 구조로, 핵심은 Self-Attention입니다.
입력 토큰을 Query, Key, Value로 변환해 문맥 내에서 어떤 단어를 얼마나 참고할지 계산하고,
이를 여러 층으로 쌓아 문맥을 점점 풍부하게 만듭니다.
최근 모델들은 RoPE(회전 위치 임베딩), GQA, MoE 같은 기법으로 긴 문맥 처리와 추론 효율을 개선하고 있습니다.

RoPE(Rotary Position Embedding): 위치 정보를 토큰 임베딩에 통합하는 방식으로,
토큰 간 상대적 위치 관계를 더 잘 반영해 긴 문맥에서도 성능 저하를 줄이는 기법입니다.
Grouped Query Attention: 여러개의 쿼리 헤드들을 몇 개의 그룹으로 묶고, 각 그룹이 단일 헤드처럼 동작하게 하여
메모리 사용량을 줄이면서도 성능 저하를 최소화하는 기법입니다.
MoE(Mixture of Experts): 모델 내에 여러 개의 전문가 네트워크를 두고, 입력에 따라 가장 적합한 전문가들만 활성화하여
효율적으로 모델 용량을 확장하는 방법입니다.

✅ 2. Fine-tuning / LoRA / QLoRA (통합)
❓ Q. Fine-tuning과 LoRA의 차이, 그리고 LoRA rank의 의미는?
✅ 정리된 답변

Fine-tuning은 모델 전체 파라미터를 업데이트하는 방식으로 성능은 좋지만
GPU 메모리 사용량과 비용이 큽니다.

LoRA는 기존 가중치를 고정한 채 저랭크 행렬을 추가해
일부 파라미터만 학습하는 방식으로,
메모리 효율이 높고 여러 버전의 모델을 관리하기에 적합합니다.

LoRA의 rank 값은 학습 표현력을 결정하며,
rank를 높이면 성능은 좋아질 수 있지만
파라미터 수와 메모리 사용량도 함께 증가하는 트레이드오프가 있습니다.

QLoRA 장단점(여기에 “양자화로 VRAM 절약” 한 줄 추가하면 끝)

Fine-tuning할 때 가장 중요하다고 생각하는 요소는?
데이터 품질과 지시문의 통일성이 가장 중요하다고 생각합니다.
트랜스포머 기반 모델은 패턴 학습에 민감하기 때문에, 지시문과 답변 구조가 일관되지 않으면 성능이 크게 떨어집니다.
또한 데이터 수가 적을 때는 학습률을 낮게 하고, LoRA로 효율적으로 학습해야 안정적으로 수렴했습니다.

✅ 3. 모델 학습 디버깅 / 성능 개선 (통합)
❓ Q. 모델 성능이 안 오를 때 어떻게 접근하나요?
✅ 정리된 답변

저는 아래 순서로 접근했습니다.

데이터 품질 확인 (라벨, 입력 형식, 번역 품질)

학습 설정 점검 (learning rate, LoRA rank, epoch)

모델 비교 (다른 구조나 baseline과 출력 비교)

데이터 증강 시도(T5, synatra) 한국어>영어>한국어

마지막으로 모델 구조나 파라미터 수정

실제 프로젝트에서도 파라미터 조정보다
데이터 증강과 입력 품질 개선이 성능 향상에 가장 큰 영향을 줬습니다.

✅ 4. 데이터가 적을 때 성능 올리는 방법 (통합)
❓ Q. 데이터가 적을 때 성능을 어떻게 올릴 수 있나요?
✅ 정리된 답변

데이터 증강을 통해 데이터 다양성을 늘리는 것이 가장 효과적이었습니다.
다양성을 증가시켜 과적합을 방지했다
실제로 번역–역번역 방식으로 같은 의미지만 표현이 다른 문장을 만들어
모델이 특정 패턴에 과적합되지 않도록 했습니다.

또한 데이터 수가 적을 때는
learning rate를 낮추고 LoRA처럼 파라미터 효율적인 방식이 안정적이었습니다.

overfitting 해결?
데이터 증강, dropout, early stopping을 활용했고
파인튜닝 시에는 학습 가능한 파라미터를 제한해 과적합을 줄였습니다.

✅ 5. GPU 메모리 / 추론 속도 (통합)
❓ Q. GPU 메모리가 부족하거나 추론이 느릴 때는 어떻게 하나요?
✅ 정리된 답변

학습 시에는 batch size 감소, gradient accumulation(작은 batch 여러 번 처리 후 합산), QLoRA(LoRA + 양자화)를 활용해
메모리 사용량을 줄였습니다.

추론 단계에서는 batch 처리, prompt 길이 축소,
필요하면 추론 특화 모델로 교체하는 방식이 효과적이라고 생각합니다.

적은 VRAM으로 Fine-tuning이 가능, 실제 서비스에서는 큰 차이가 없고, 메모리 사용량과 비용 절감 효과가 커 사용한다고 알고 있습니다.
서비스에서는 성능과 속도의 균형이 가장 중요하다고 봅니다.

✅ 6. Diffusion Model (통합)
❓ Q. Diffusion Model을 설명해보세요. GAN과 비교하면?
✅ 정리된 답변

Diffusion Model은 데이터를 점진적으로 노이즈화한 뒤,
그 과정을 거꾸로 학습해 원본을 복원하며 생성하는 모델입니다.

GAN에 비해 학습이 안정적이고 모드 붕괴 문제가 적어
고해상도 이미지나 스타일 일관성이 중요한 분야에서 선호됩니다.

Stable Diffusion은 이를 latent space에서 수행해 효율을 높였습니다.

✅ 7. 멀티모달 / VQA / CLIP (통합)
❓ Q. 멀티모달 모델이 어려운 이유와 VQA 구조를 설명해보세요.
✅ 정리된 답변

이미지와 텍스트는 분포와 표현 방식이 완전히 다르기 때문에
두 modality를 같은 latent 공간으로 정렬하는 것이 어렵습니다.

CLIP처럼 이미지 인코더와 텍스트 인코더를 각각 학습시킨 뒤
공통 임베딩 공간에서 alignment를 맞추는 방식이 대표적입니다.

VQA에서는 이 정렬 품질과 attention 구조가 성능의 핵심이라고 생각합니다.

✅ 8. Docker / AWS / MLOps (통합)
❓ Q. Docker와 AWS를 활용한 모델 배포 구조를 설명해보세요.
✅ 정리된 답변

Docker는 개발과 배포 환경을 동일하게 유지해
재현성과 안정성을 확보하기 위해 사용합니다.

일반적으로 모델 weight는 S3에 저장하고,
FastAPI 기반 추론 서버를 Docker로 컨테이너화해
GPU EC2에서 실행하는 구조를 사용합니다.

EC2 (GPU) → 모델 서버(FastAPI, Triton)
S3 → 모델 checkpoint 저장
ECR + Docker → 배포 버전 관리
Load Balancer → 트래픽 분산
CloudWatch → 모니터링 및 알람
IAM → 권한 관리

이미지는 ECR로 버전 관리하고,
로그와 모니터링은 CloudWatch로 확인합니다.

아직 실무 경험은 없지만, 이 구조를 이해하고 빠르게 적응하는 것이
신입으로서의 역할이라고 생각합니다.

✅ 9. 웹툰 서비스 관점 (통합)
❓ Q. 웹툰 서비스를 위한 AI 모델에서 무엇을 최적화해야 하나요?
✅ 정리된 답변

품질, 추론 속도, 비용, 안정성 네 가지가 핵심이라고 생각합니다.

특히 웹툰은 컷 간 스타일 일관성과 반복 생성 안정성이 중요하고,
창작자가 기다리지 않도록 속도도 중요합니다.

AI는 창작자를 대체하기보다
반복 작업을 줄여주는 도구여야 한다고 생각합니다.

LLM은 스토리 보조, 대사 생성, 설정 정리에 적합하고
Diffusion은 캐릭터·배경·컷 이미지 생성에 적합합니다.
웹툰에서는 두 모델을 분리해 쓰기보다 멀티모달 파이프라인으로 결합하는 것이 중요하다고 생각합니다.

--------------------------------------------------------------------------------------------------
최근 사용해본 모델 중 구조적으로 인상 깊었던 모델은?

최근 사용해본 모델 중에서는 GPT-5 계열이 구조적으로 인상 깊었습니다.
내부 아키텍처가 공개된 것은 아니지만, 실제 사용하면서
멀티모달 입력을 자연스럽게 통합하고, 긴 컨텍스트에서도
추론 흐름이 크게 흔들리지 않는 점에서 기존 Transformer 구조 대비 설계 개선이 느껴졌습니다.
--------------------------------------------------------------------------------------------------
1) Cross-Entropy가 뭐고 왜 쓰나요? 
크로스 엔트로피 손실 함수를 사용하는 주된 이유는 모델의 예측이 실제와 얼마나 잘 일치하는지를 정확하게 측정할 수 있기 때문입니다.
2)Batch size 늘리면?
전체 데이터셋을 여러 작은 묶음(배치)으로 나누어 학습하며, 배치 크기가 클수록 GPU 연산 효율은 높지만 메모리 부담이 커지고,
작을수록 안정적이지만 학습이 느려질 수 있어 속도와 성능 간의 균형을 맞추는 것이 중요합니다.
3)LR scheduler
학습률 스케줄러는 모델 성능과 학습 안정성에 큰 영향(에폭마다 수정 or 정해진 패턴 or 개선되지 않을 때)
4)Pretrain과 FT 차이
Pre-training (사전 학습):
모델이 처음 '세상'의 언어나 이미지 패턴을 배우는 과정입니다.
Fine-tuning (파인 튜닝):
사전 학습된 모델을 가져와 특정 목표에 맞게 추가 학습시키는 과정입니다.
5)서비스에서 추론 속도가 중요한 이유
추론 속도는 사용자 체감 품질, 비용 효율, 시스템 확장성, 비즈니스 성과 모두에 직결되는 필수 성능 지표다.
6)사용자 UX 개선 관점에서 AI가 기여할 부분
UX 관점에서 AI의 가장 큰 기여는 기능을 늘리는 게 아니라,
사용자의 입력·탐색·판단 부담을 줄여서 더 빠르고 자연스럽게 목표에 도달하게 만드는 데 있다고 생각합니다.
-------------------------요구역량--------------------------------
1. 최신 AI 모델에 대한 심층적 이해:
LLM은 Transformer 기반 구조로, 핵심은 Self-Attention입니다.
입력 토큰을 Query, Key, Value로 변환해 문맥 내에서 어떤 단어를 얼마나 참고할지 계산하고,
이를 여러 층으로 쌓아 문맥을 점점 풍부하게 만듭니다.
최근 모델들은 RoPE(회전 위치 임베딩), GQA, MoE 같은 기법으로 긴 문맥 처리와 추론 효율을 개선하고 있습니다.

2. AI 개발 핵심 기술 숙련도:
Python으로 AI 개발할 때 중요하게 생각하는 점은?

가독성과 재현성입니다.
실험이 반복되기 때문에 코드 구조를 단순하게 유지하고,
실험 설정을 쉽게 바꿀 수 있도록 관리하는 것이 중요하다고 생각합니다.

PyTorch로 모델 학습 시 기본 흐름을 설명해보세요.

Dataset과 DataLoader로 데이터를 구성하고,
모델 정의 후 loss와 optimizer를 설정합니다.
학습 루프에서 forward → loss 계산 → backward(역전파) → optimizer step을 반복하며,
validation 단계에서는 torch.no_grad()로 성능을 평가합니다.

3. 효율적인 모델 운영 및 배포 능력:
Docker는 개발과 배포 환경을 동일하게 유지해
재현성과 안정성을 확보하기 위해 사용합니다.

일반적으로 모델 weight는 S3에 저장하고,
FastAPI 기반 추론 서버를 Docker로 컨테이너화해
GPU EC2에서 실행하는 구조를 사용합니다.

EC2 (GPU) → 모델 서버(FastAPI, Triton)
S3 → 모델 checkpoint 저장
ECR + Docker → 배포 버전 관리
Load Balancer → 트래픽 분산
CloudWatch → 모니터링 및 알람
IAM → 권한 관리

이미지는 ECR로 버전 관리하고,
로그와 모니터링은 CloudWatch로 확인합니다.

아직 실무 경험은 없지만, 이 구조를 이해하고 빠르게 적응하는 것이
신입으로서의 역할이라고 생각합니다.

MLOps 핵심 요소인 자동화, 효율성, 안정성, 재현성 중 재현성이 가장 중요하다고 생각합니다.
재현성이 확보되어야만 모델 성능을 일관되게 유지하고, 문제 발생 시 신속하게 원인을 파악할 수 있기 때문입니다.

자동화: 데이터 수집, 전처리, 모델 학습, 평가, 배포 등 전체 파이프라인 자동화.
효율성: 모델 개발 및 배포 시간을 단축하고 자원을 효율적으로 사용.
안정성: 모델의 성능 저하를 지속적으로 모니터링하고 신속하게 대응.
재현성: 실험 환경, 데이터, 코드의 버전을 관리하여 재현 가능한 결과 보장.
